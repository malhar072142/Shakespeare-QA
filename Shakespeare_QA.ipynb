{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# IE 7500 Applied NLP"
      ],
      "metadata": {
        "id": "FaogJWV6qEzI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question and Answer Bot using Pretrained BERT"
      ],
      "metadata": {
        "id": "aGGMiVWkqIy7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final Exam"
      ],
      "metadata": {
        "id": "FHB00yRop_95"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Data Preprocessing"
      ],
      "metadata": {
        "id": "MdNyVr3IqNkB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pdfplumber PyPDF2 gradio"
      ],
      "metadata": {
        "id": "20Ivz_3fk7_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1. Preprocessing and Cleaning"
      ],
      "metadata": {
        "id": "44EivMIMqeNx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from transformers import pipeline\n",
        "from PyPDF2 import PdfReader\n",
        "import spacy\n",
        "import gradio as gr"
      ],
      "metadata": {
        "id": "BRlKYOan0CVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Extract text from the PDF\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    reader = PdfReader(pdf_path)\n",
        "    text = \"\"\n",
        "    for page in reader.pages:\n",
        "        text += page.extract_text()\n",
        "    return text\n",
        "\n",
        "# Step 2: Clean the text\n",
        "def clean_text(raw_text):\n",
        "    # Basic cleaning: remove headers, footers, and excessive whitespace\n",
        "    text = re.sub(r\"\\n{2,}\", \"\\n\", raw_text)  # Replace multiple newlines with a single newline\n",
        "    text = re.sub(r\"\\s{2,}\", \" \", text)  # Replace multiple spaces with a single space\n",
        "    text = re.sub(r\"\\b\\d+\\b\", \"\", text)  # Remove isolated numbers (like page numbers)\n",
        "    # Retain only paragraphs with valid content\n",
        "    paragraphs = [\n",
        "        para.strip() for para in text.split(\"\\n\")\n",
        "        if len(para.strip()) > 20 and re.search(r\"^[A-Za-z]\", para)\n",
        "    ]\n",
        "    return \" \".join(paragraphs)\n",
        "\n",
        "    # Step 3: Remove irrelevant sections using stricter filtering\n",
        "    paragraphs = [\n",
        "        para for para in paragraphs\n",
        "        if not re.search(r\"(COPYRIGHT|PROJECT GUTENBERG|ELECTRONIC VERSION|SERVICE|DOWNLOAD|MEMBERSHIP|PERMISSION|MACHINE READABLE|PROHIBITED DISTRIBUTION)\", para, re.IGNORECASE)\n",
        "        and not re.search(r\"^\\s*(DISTRIBUTED|PERSONAL USE ONLY|COMMERCIALLY)\", para, re.IGNORECASE)  # Exclude specific patterns\n",
        "        and len(para.strip()) > 20  # Exclude short lines likely to be non-content\n",
        "    ]\n",
        "    return \" \".join(paragraphs)\n",
        "\n",
        "    # Step 4: Retain only paragraphs with recognizable Shakespearean content\n",
        "    paragraphs = [\n",
        "        para for para in paragraphs\n",
        "        if re.search(r\"^[A-Za-z]\", para)  # Starts with letters\n",
        "    ]\n",
        "\n",
        "    return \" \".join(paragraphs)\n",
        "\n",
        "\n",
        "\n",
        "# Main Implementation\n",
        "pdf_path = \"/content/Shakespeare-Complete-Works.pdf\"  # Update this path as needed\n",
        "raw_text = extract_text_from_pdf(pdf_path)\n",
        "cleaned_text = clean_text(raw_text)\n"
      ],
      "metadata": {
        "id": "W4BnmC4fk84Y"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Clean the Extracted Text\n",
        "\n",
        "- **Function**: `clean_text(raw_text)`\n",
        "- **Input**: Raw text extracted from the PDF.\n",
        "- **Output**: Cleaned and formatted text as a single string.\n",
        "\n",
        "#### Cleaning Steps:\n",
        "1. **Remove Excessive Newlines**: Replace multiple consecutive newlines with a single newline.\n",
        "2. **Remove Excessive Whitespace**: Replace multiple consecutive spaces with a single space.\n",
        "3. **Remove Isolated Numbers**: Exclude numbers like page numbers that appear on their own.\n",
        "4. **Filter Valid Paragraphs**: Retain only paragraphs that:\n",
        "   - Have meaningful content (at least 20 characters).\n",
        "   - Start with letters (ignore any lines starting with non-alphabetic characters).\n",
        "\n",
        "This ensures the output is clean and structured for further processing.\n",
        "\n",
        "- The code is structured to:\n",
        "  - Extract raw text from a PDF file.\n",
        "  - Clean and preprocess the extracted text for further analysis.\n",
        "  \n",
        "- After this we will:\n",
        "  - Segment the cleaned text into smaller chunks.\n",
        "  - Use the cleaned text as input to a question-answering pipeline.\n",
        "  \n"
      ],
      "metadata": {
        "id": "KU1zesmLqdAS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2. Data Segmention into smaller logical units"
      ],
      "metadata": {
        "id": "Hm5lpiwQs-x4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "# Step 3: Segment the text\n",
        "def segment_text(text, segment_size=500):\n",
        "    return [text[i:i + segment_size] for i in range(0, len(text), segment_size)]\n",
        "\n",
        "# Step 4: Preprocess and Cache NER Entities\n",
        "def extract_entities(text, nlp):\n",
        "    doc = nlp(text)\n",
        "    return {ent.text.lower() for ent in doc.ents}\n",
        "\n",
        "def preprocess_segments_with_entities(segments, nlp):\n",
        "    # Cache entities for all segments\n",
        "    segment_entities = []\n",
        "    for segment in segments:\n",
        "        entities = extract_entities(segment, nlp)\n",
        "        segment_entities.append((segment, entities))\n",
        "    return segment_entities\n",
        "\n",
        "# Step 5: Find Relevant Segment Using Preprocessed Entities\n",
        "def find_relevant_segment(question, segment_entities, nlp):\n",
        "    question_doc = nlp(question)\n",
        "    question_entities = {ent.text.lower() for ent in question_doc.ents}\n",
        "\n",
        "    best_segment = \"\"\n",
        "    max_matches = 0\n",
        "    for segment, entities in segment_entities:\n",
        "        matches = len(question_entities.intersection(entities))\n",
        "        if matches > max_matches:\n",
        "            max_matches = matches\n",
        "            best_segment = segment\n",
        "    return best_segment"
      ],
      "metadata": {
        "id": "tcI1BXdslBa5"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Segment the Text**\n",
        "- **Function**: `segment_text(text, segment_size=500)`\n",
        "- **Purpose**: Break the cleaned text into smaller, fixed-sized chunks (default size: 500 characters).\n",
        "- **Input**: The cleaned text.\n",
        "- **Output**: A list of text segments.\n",
        "- **Why?**: Processing smaller segments ensures efficient handling during Named Entity Recognition (NER) and Question-Answering tasks.\n",
        "\n",
        "\n",
        "#### **Step 4: Preprocess and Cache NER Entities**\n",
        "1. **Entity Extraction**:\n",
        "   - **Function**: `extract_entities(text, nlp)`\n",
        "   - **Purpose**: Extract Named Entities (e.g., names, places, dates) from a given text segment using a spaCy NLP model.\n",
        "   - **Input**: A single text segment and a spaCy NLP object (`nlp`).\n",
        "   - **Output**: A set of extracted entity names in lowercase.\n",
        "\n",
        "2. **Caching Entities for Segments**:\n",
        "   - **Function**: `preprocess_segments_with_entities(segments, nlp)`\n",
        "   - **Purpose**: For each text segment, extract entities and store them in a list for faster access during the relevance matching step.\n",
        "   - **Input**: A list of text segments and a spaCy NLP object.\n",
        "   - **Output**: A list of tuples, where each tuple contains:\n",
        "     - The text segment.\n",
        "     - The set of named entities extracted from that segment.\n",
        "\n",
        "\n",
        "#### **Find Relevant Segment Using Preprocessed Entities**\n",
        "- **Function**: `find_relevant_segment(question, segment_entities, nlp)`\n",
        "- **Purpose**: Match a user's question to the most relevant text segment based on the overlap of named entities.\n",
        "\n",
        "**How it Works**:\n",
        "1. **Extract Entities from the Question**:\n",
        "   - Uses spaCy to extract entities from the user's question.\n",
        "\n",
        "2. **Match Entities with Cached Segment Entities**:\n",
        "   - Compares the entities extracted from the question to the entities cached for each segment.\n",
        "   - Calculates the number of matching entities between the question and each segment.\n",
        "\n",
        "3. **Select the Best Segment**:\n",
        "   - The segment with the highest number of matching entities is considered the most relevant.\n",
        "\n",
        "**Input**:\n",
        "   - The user's question (string).\n",
        "   - The list of preprocessed `segment_entities`.\n",
        "   - The spaCy NLP object.\n",
        "\n",
        "**Output**:\n",
        "   - The most relevant text segment (string).\n",
        "\n"
      ],
      "metadata": {
        "id": "_b-H0UgsrMzg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess segments\n",
        "segments = segment_text(cleaned_text)\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "segment_entities = preprocess_segments_with_entities(segments, nlp)"
      ],
      "metadata": {
        "id": "mFbyvk5tlFsL"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing Segments and Extracting Entities\n",
        "\n",
        "This code preprocesses the cleaned text to prepare it for efficient question-answering by segmenting the text and extracting named entities from each segment.\n",
        "\n",
        "#### **Code Breakdown**\n",
        "1. **Segmenting the Text**:\n",
        "   - **`segment_text(cleaned_text)`**:\n",
        "     - Divides the cleaned text into smaller chunks of a fixed size (default: 500 characters).\n",
        "     - This ensures each segment is manageable for further processing, such as Named Entity Recognition (NER) and matching.\n",
        "\n",
        "2. **Loading spaCy's NLP Model**:\n",
        "   - **`spacy.load(\"en_core_web_sm\")`**:\n",
        "     - Loads a pre-trained spaCy model capable of performing NER.\n",
        "     - The model identifies named entities (e.g., people, locations, dates) in each segment.\n",
        "\n",
        "3. **Extracting and Caching Named Entities**:\n",
        "   - **`preprocess_segments_with_entities(segments, nlp)`**:\n",
        "     - For each text segment:\n",
        "       1. Extracts named entities using the spaCy model.\n",
        "       2. Stores the segment and its corresponding entities as a tuple.\n",
        "     - Results in a list called `segment_entities`, where each entry contains:\n",
        "       - The text segment.\n",
        "       - A set of named entities found in the segment.\n",
        "\n",
        "#### **Advantages of Using This Process**\n",
        "1. **Efficiency**:\n",
        "   - By segmenting the text and precomputing named entities, we avoid reprocessing the entire text every time a question is asked.\n",
        "   - This significantly reduces runtime for matching questions with relevant text segments.\n",
        "\n",
        "2. **Scalability**:\n",
        "   - Handles large texts (e.g., Shakespeare's Complete Works) by working with smaller, more manageable chunks.\n",
        "   - Enables processing of extensive datasets without running out of memory.\n",
        "\n",
        "3. **Accurate Context Matching**:\n",
        "   - Named Entity Recognition (NER) ensures questions are matched with text segments containing relevant entities (e.g., \"Bertram\" or \"Rousillon\").\n",
        "   - Improves the accuracy of the question-answering system by narrowing down the context.\n",
        "\n",
        "4. **Reusability**:\n",
        "   - Preprocessed segments and their entities are stored and reused, making the system efficient for multiple questions without recalculating entities.\n",
        "\n",
        "5. **Flexibility**:\n",
        "   - The process can handle a variety of texts and adapt to different domains or datasets by using spaCy's customizable NER models.\n"
      ],
      "metadata": {
        "id": "sCYwVIbGrNQm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Model Selection and Setup"
      ],
      "metadata": {
        "id": "u2Llo6WHtMNC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Set up the Hugging Face Q&A pipeline\n",
        "def setup_pipeline():\n",
        "    return pipeline(\"question-answering\", model=\"distilbert-base-uncased\", tokenizer=\"distilbert-base-uncased\")\n",
        "\n",
        "# Step 4: Ask questions and get answers\n",
        "def ask_question(qa_pipeline, context, question):\n",
        "    result = qa_pipeline(question=question, context=context)\n",
        "    return result[\"answer\"]"
      ],
      "metadata": {
        "id": "KkY2JehplJhH"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting Up the Hugging Face Q&A Pipeline\n",
        "\n",
        "This section sets up the question-answering (Q&A) pipeline using the Hugging Face Transformers library and defines a function to interact with the pipeline for retrieving answers to user queries.\n",
        "\n",
        "#### **Set Up the Q&A Pipeline**\n",
        "- **Function**: `setup_pipeline()`\n",
        "- **Purpose**:\n",
        "  - Initializes the Hugging Face Q&A pipeline.\n",
        "  - Uses the pre-trained `distilbert-base-uncased` model and tokenizer for efficient question-answering tasks.\n",
        "- **Key Components**:\n",
        "  - **Model**: `distilbert-base-uncased`:\n",
        "    - A lightweight, pre-trained Transformer model optimized for speed and accuracy.\n",
        "  - **Pipeline**: Configures the question-answering workflow.\n",
        "\n",
        "**Output**:\n",
        "- Returns a pipeline object that can process user questions and provide answers based on a given context.\n",
        "\n",
        "\n",
        "#### **Ask Questions and Get Answers**\n",
        "- **Function**: `ask_question(qa_pipeline, context, question)`\n",
        "- **Purpose**:\n",
        "  - Interacts with the initialized Q&A pipeline to process user queries and retrieve answers.\n",
        "- **Parameters**:\n",
        "  - **`qa_pipeline`**: The Hugging Face pipeline initialized by `setup_pipeline()`.\n",
        "  - **`context`**: A text segment containing relevant information for the question.\n",
        "  - **`question`**: The user’s question as a string.\n",
        "- **Workflow**:\n",
        "  1. Passes the question and context to the Q&A pipeline.\n",
        "  2. Extracts and returns the answer from the pipeline’s output.\n",
        "\n",
        "**Output**:\n",
        "- Returns the answer extracted from the context for the given question.\n",
        "\n",
        "\n",
        "1. **Pre-Trained Model**:\n",
        "   - The use of `distilbert-base-uncased` leverages transfer learning, allowing the system to handle questions without requiring extensive training.\n",
        "\n",
        "2. **Seamless Integration**:\n",
        "   - The pipeline simplifies the interaction between the model and the user, abstracting away lower-level implementation details.\n",
        "\n",
        "3. **Scalability**:\n",
        "   - Works efficiently with the segmented and preprocessed text to handle large datasets like Shakespeare’s works.\n",
        "\n",
        "4. **User-Friendly**:\n",
        "   - By abstracting the Q&A task into a reusable function, the system becomes modular and easy to extend or modify.\n",
        "\n"
      ],
      "metadata": {
        "id": "_XT1NhU5rihv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up the pipeline\n",
        "qa_pipeline = setup_pipeline()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "laF-Ss5_lMQJ",
        "outputId": "27667bc1-be6c-49f7-e322-61f115b03ddb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Designing Q A system"
      ],
      "metadata": {
        "id": "pokgHyietQEQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "example_question = \"Who is Bertram?\"\n",
        "relevant_segment = find_relevant_segment(example_question, segment_entities, nlp)\n",
        "if relevant_segment:\n",
        "    answer = qa_pipeline(question=example_question, context=relevant_segment)\n",
        "    print(f\"Question: {example_question}\")\n",
        "    print(f\"Answer: {answer['answer']}\")\n",
        "else:\n",
        "    print(\"No relevant segment found.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XDdGgzsilQIu",
        "outputId": "dd13c374-6882-419a-9ec7-20540471591c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: Who is Bertram?\n",
            "Answer: and Servant to the Countess of Rousillon\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Main Q&A Function\n",
        "def shakespeare_qa(question):\n",
        "\n",
        "    try:\n",
        "        answer = qa_pipeline(question=question, context=relevant_segment)\n",
        "        return answer['answer']\n",
        "    except Exception as e:\n",
        "        return f\"Sorry, I couldn't find an answer. Error: {str(e)}\""
      ],
      "metadata": {
        "id": "dLkLnsAjlTJx"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Web Interface Implementation for Q A system"
      ],
      "metadata": {
        "id": "DE46F3b8tVyC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "id": "V5nj12IUlZl_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "# Create Gradio Interface\n",
        "iface = gr.Interface(\n",
        "    fn=shakespeare_qa,\n",
        "    inputs=gr.Textbox(label=\"Ask a question about Shakespeare's works\"),\n",
        "    outputs=gr.Textbox(label=\"Answer\"),\n",
        "    title=\"Shakespeare Works Q&A\",\n",
        "    description=\"Ask questions about characters, plots, and themes in Shakespeare's works.\",\n",
        "    examples=[\n",
        "        \"Who is Bertram?\",\n",
        "        \"What happens in Romeo and Juliet?\",\n",
        "        \"Describe Macbeth's character\",\n",
        "        \"Who wrote these plays?\"\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Launch the interface\n",
        "iface.launch(share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "id": "LgaqnyiylU4o",
        "outputId": "27fe79e0-54bc-4368-96dc-5444c6eb780f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://93d7e101e6c4f4125b.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://93d7e101e6c4f4125b.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}